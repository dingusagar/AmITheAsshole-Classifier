{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "assert torch.cuda.is_available(), \"CUDA NOT FOUND\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "train = pd.read_csv('../data/TRAIN.csv').drop('Unnamed: 0', axis=1)\n",
    "test = pd.read_csv('../data/TEST.csv').drop('Unnamed: 0', axis=1)\n",
    "validation = pd.read_csv('../data/VALIDATION.csv').drop('Unnamed: 0', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['label'] = 1 - train['label']\n",
    "test['label'] = 1 - test['label']\n",
    "validation['label'] = 1 - validation['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "\n",
    "X = train['text'].to_frame()\n",
    "y = train['label']\n",
    "\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "X_resampled, y_resampled = ros.fit_resample(X, y)\n",
    "\n",
    "train_balanced_oversampled = pd.concat([X_resampled, y_resampled], axis=1)\n",
    "train_balanced_oversampled.columns = ['text', 'label']\n",
    "train_balanced_oversampled.reset_index(inplace=True, drop='index')\n",
    "\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "X_resampled, y_resampled = rus.fit_resample(X, y)\n",
    "\n",
    "train_balanced_undersampled = pd.concat([X_resampled, y_resampled], axis=1)\n",
    "train_balanced_undersampled.columns = ['text', 'label']\n",
    "train_balanced_undersampled.reset_index(inplace=True, drop='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import (\n",
    "    Dataset,\n",
    "    DatasetDict\n",
    ")\n",
    "\n",
    "\n",
    "# oversampling works better\n",
    "tds = Dataset.from_pandas(train_balanced_oversampled)\n",
    "testds = Dataset.from_pandas(test)\n",
    "vds = Dataset.from_pandas(validation)\n",
    "dataset = DatasetDict()\n",
    "dataset['train'] = tds\n",
    "dataset['test'] = testds\n",
    "dataset['validation'] = vds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b053357df8f46158faecb017bcaa75f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/150382 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8eb23dbaf49a4647961eeda518371882",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12396 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a87204dbcc244a9e8d74cb604baf5cd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12395 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def tokenize(batch):\n",
    "    tokenized = tokenizer(batch[\"text\"], padding=True, truncation=True)\n",
    "    tokenized['label'] = batch['label']\n",
    "    return tokenized\n",
    "\n",
    "dataset_encoded = dataset.map(tokenize, batched=True, batch_size=64, remove_columns=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30124987c27d4c5fb05dd0945cc9bf74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/150382 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afddffd539df4438be02a4acf8ae023d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/12396 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbae6335493a4a3fb2ad11e991b5b72c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/12395 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_encoded.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "dataset_encoded.save_to_disk('../data/dataset_encoded_oversampled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertForSequenceClassification\n",
    "\n",
    "\n",
    "model = DistilBertForSequenceClassification.from_pretrained(model_name, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from transformers import (\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "\n",
    "\n",
    "def train(dataset, model, training_args):\n",
    "\n",
    "    def compute_metrics(eval_pred):\n",
    "        predictions, labels = eval_pred\n",
    "        predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "        metrics = {}\n",
    "        metrics['accuracy'] = (predictions == labels).mean()\n",
    "\n",
    "        tp = np.sum((predictions == 1) & (labels == 1))\n",
    "        fp = np.sum((predictions == 1) & (labels == 0))\n",
    "        fn = np.sum((predictions == 0) & (labels == 1))\n",
    "\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "        metrics['precision'] = precision\n",
    "        metrics['recall'] = recall\n",
    "        metrics['f1'] = f1\n",
    "\n",
    "        return metrics\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=dataset['train'],\n",
    "        eval_dataset=dataset['validation'],\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    return trainer.model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./aita_classifier\",\n",
    "    learning_rate=2e-5,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=64,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=200,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=200,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    push_to_hub=False,\n",
    "    logging_steps=50,\n",
    "    report_to=[\"tensorboard\"],\n",
    "    fp16=torch.cuda.is_available(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7050' max='7050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7050/7050 14:19, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.660900</td>\n",
       "      <td>0.661676</td>\n",
       "      <td>0.592416</td>\n",
       "      <td>0.821622</td>\n",
       "      <td>0.584782</td>\n",
       "      <td>0.683260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.659300</td>\n",
       "      <td>0.666527</td>\n",
       "      <td>0.582170</td>\n",
       "      <td>0.837657</td>\n",
       "      <td>0.550977</td>\n",
       "      <td>0.664725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.654000</td>\n",
       "      <td>0.649619</td>\n",
       "      <td>0.620976</td>\n",
       "      <td>0.835074</td>\n",
       "      <td>0.617836</td>\n",
       "      <td>0.710215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.629900</td>\n",
       "      <td>0.639475</td>\n",
       "      <td>0.620653</td>\n",
       "      <td>0.830944</td>\n",
       "      <td>0.621915</td>\n",
       "      <td>0.711392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.633900</td>\n",
       "      <td>0.626119</td>\n",
       "      <td>0.625494</td>\n",
       "      <td>0.838252</td>\n",
       "      <td>0.621807</td>\n",
       "      <td>0.713986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.629500</td>\n",
       "      <td>0.678063</td>\n",
       "      <td>0.571359</td>\n",
       "      <td>0.858293</td>\n",
       "      <td>0.514810</td>\n",
       "      <td>0.643590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.611300</td>\n",
       "      <td>0.658705</td>\n",
       "      <td>0.611214</td>\n",
       "      <td>0.854308</td>\n",
       "      <td>0.582099</td>\n",
       "      <td>0.692411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.603500</td>\n",
       "      <td>0.581458</td>\n",
       "      <td>0.679064</td>\n",
       "      <td>0.825292</td>\n",
       "      <td>0.726980</td>\n",
       "      <td>0.773023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.607100</td>\n",
       "      <td>0.605404</td>\n",
       "      <td>0.654296</td>\n",
       "      <td>0.836565</td>\n",
       "      <td>0.671281</td>\n",
       "      <td>0.744865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.582400</td>\n",
       "      <td>0.600975</td>\n",
       "      <td>0.666478</td>\n",
       "      <td>0.835838</td>\n",
       "      <td>0.692316</td>\n",
       "      <td>0.757337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.607000</td>\n",
       "      <td>0.592758</td>\n",
       "      <td>0.667527</td>\n",
       "      <td>0.834815</td>\n",
       "      <td>0.695321</td>\n",
       "      <td>0.758710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.533700</td>\n",
       "      <td>0.607708</td>\n",
       "      <td>0.669060</td>\n",
       "      <td>0.827885</td>\n",
       "      <td>0.706697</td>\n",
       "      <td>0.762506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.524300</td>\n",
       "      <td>0.645956</td>\n",
       "      <td>0.645664</td>\n",
       "      <td>0.837953</td>\n",
       "      <td>0.655398</td>\n",
       "      <td>0.735517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.520100</td>\n",
       "      <td>0.612941</td>\n",
       "      <td>0.667527</td>\n",
       "      <td>0.825423</td>\n",
       "      <td>0.707341</td>\n",
       "      <td>0.761833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.507800</td>\n",
       "      <td>0.665328</td>\n",
       "      <td>0.630980</td>\n",
       "      <td>0.837028</td>\n",
       "      <td>0.632217</td>\n",
       "      <td>0.720347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.496800</td>\n",
       "      <td>0.638706</td>\n",
       "      <td>0.658088</td>\n",
       "      <td>0.826814</td>\n",
       "      <td>0.689633</td>\n",
       "      <td>0.752019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.496400</td>\n",
       "      <td>0.625861</td>\n",
       "      <td>0.658411</td>\n",
       "      <td>0.826735</td>\n",
       "      <td>0.690277</td>\n",
       "      <td>0.752369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.496100</td>\n",
       "      <td>0.661652</td>\n",
       "      <td>0.643082</td>\n",
       "      <td>0.834930</td>\n",
       "      <td>0.654647</td>\n",
       "      <td>0.733879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.475900</td>\n",
       "      <td>0.626614</td>\n",
       "      <td>0.675676</td>\n",
       "      <td>0.821403</td>\n",
       "      <td>0.726551</td>\n",
       "      <td>0.771071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.466400</td>\n",
       "      <td>0.644999</td>\n",
       "      <td>0.664139</td>\n",
       "      <td>0.823197</td>\n",
       "      <td>0.704550</td>\n",
       "      <td>0.759267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.463400</td>\n",
       "      <td>0.623051</td>\n",
       "      <td>0.683905</td>\n",
       "      <td>0.816753</td>\n",
       "      <td>0.747156</td>\n",
       "      <td>0.780406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>0.480300</td>\n",
       "      <td>0.674763</td>\n",
       "      <td>0.644050</td>\n",
       "      <td>0.835109</td>\n",
       "      <td>0.656042</td>\n",
       "      <td>0.734824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>0.453900</td>\n",
       "      <td>0.665401</td>\n",
       "      <td>0.658411</td>\n",
       "      <td>0.827831</td>\n",
       "      <td>0.688882</td>\n",
       "      <td>0.751992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.409100</td>\n",
       "      <td>0.687998</td>\n",
       "      <td>0.669302</td>\n",
       "      <td>0.824041</td>\n",
       "      <td>0.712170</td>\n",
       "      <td>0.764032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.395600</td>\n",
       "      <td>0.710677</td>\n",
       "      <td>0.661960</td>\n",
       "      <td>0.826541</td>\n",
       "      <td>0.696501</td>\n",
       "      <td>0.755970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>0.393700</td>\n",
       "      <td>0.690787</td>\n",
       "      <td>0.675837</td>\n",
       "      <td>0.816986</td>\n",
       "      <td>0.732990</td>\n",
       "      <td>0.772712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>0.373400</td>\n",
       "      <td>0.708226</td>\n",
       "      <td>0.669141</td>\n",
       "      <td>0.821918</td>\n",
       "      <td>0.714746</td>\n",
       "      <td>0.764594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>0.391400</td>\n",
       "      <td>0.711718</td>\n",
       "      <td>0.666478</td>\n",
       "      <td>0.821508</td>\n",
       "      <td>0.710775</td>\n",
       "      <td>0.762140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>0.399200</td>\n",
       "      <td>0.722589</td>\n",
       "      <td>0.659298</td>\n",
       "      <td>0.826143</td>\n",
       "      <td>0.692531</td>\n",
       "      <td>0.753459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.389400</td>\n",
       "      <td>0.707772</td>\n",
       "      <td>0.664946</td>\n",
       "      <td>0.822289</td>\n",
       "      <td>0.707126</td>\n",
       "      <td>0.760372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>0.366300</td>\n",
       "      <td>0.726967</td>\n",
       "      <td>0.658653</td>\n",
       "      <td>0.824633</td>\n",
       "      <td>0.693389</td>\n",
       "      <td>0.753338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>0.371900</td>\n",
       "      <td>0.718626</td>\n",
       "      <td>0.663816</td>\n",
       "      <td>0.822139</td>\n",
       "      <td>0.705409</td>\n",
       "      <td>0.759314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>0.398500</td>\n",
       "      <td>0.714090</td>\n",
       "      <td>0.669221</td>\n",
       "      <td>0.821543</td>\n",
       "      <td>0.715390</td>\n",
       "      <td>0.764800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>0.376300</td>\n",
       "      <td>0.712840</td>\n",
       "      <td>0.668899</td>\n",
       "      <td>0.821455</td>\n",
       "      <td>0.714960</td>\n",
       "      <td>0.764517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.380600</td>\n",
       "      <td>0.710510</td>\n",
       "      <td>0.670754</td>\n",
       "      <td>0.820777</td>\n",
       "      <td>0.719038</td>\n",
       "      <td>0.766547</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model, tokenizer = train(dataset_encoded, model, training_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Clean and preprocess text\"\"\"\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove multiple spaces\n",
    "    text = ' '.join(text.split())\n",
    "    # Remove special characters but keep punctuation\n",
    "    text = ''.join([c if c.isalnum() or c.isspace() or c in '.,!?' else ' ' for c in text])\n",
    "    return text\n",
    "\n",
    "\n",
    "def predict_verdict(text, model, tokenizer):\n",
    "    \"\"\"Predict verdict with preprocessing\"\"\"\n",
    "    # Preprocess text\n",
    "    clean_text = preprocess_text(text)\n",
    "\n",
    "    # Add special tokens\n",
    "    text_with_tokens = f\"[TITLE] {clean_text}\"\n",
    "\n",
    "    # Tokenize\n",
    "    inputs = tokenizer(\n",
    "        text_with_tokens,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    # Move to GPU if available\n",
    "    if torch.cuda.is_available():\n",
    "        inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}\n",
    "        model = model.to(\"cuda\")\n",
    "\n",
    "    # Get prediction\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "        prediction = torch.argmax(probabilities, dim=-1).item()\n",
    "        confidence = probabilities[0][prediction].item()\n",
    "\n",
    "    return prediction, confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verdict: NTA (confidence: 0.98)\n"
     ]
    }
   ],
   "source": [
    "# https://www.reddit.com/r/AmItheAsshole/comments/1gwl76e/aitah_for_refusing_to_let_my_moms_boyfriend_walk/\n",
    "text = \"\"\"AITAH for refusing to let my mom’s boyfriend walk me down the aisle?\n",
    "\n",
    "I (24F) am getting married next summer, and ever since I started planning the wedding, my mom (48F) has been pushing for her boyfriend (50M) to walk me down the aisle. My dad passed away when I was 10, and my mom started dating her boyfriend about six years ago. While he’s always been nice to me, I’ve never seen him as a father figure, he came into my life when I was already an adult, and we’re friendly but not particularly close.\n",
    "\n",
    "I told my mom that I plan to walk myself down the aisle as a way to honor my independence and my dad’s memory. She got really upset, saying her boyfriend has “earned” the spot by being there for me all these years. She even accused me of disrespecting her relationship and trying to “erase” my dad, which couldn’t be further from the truth.\n",
    "\n",
    "Now her boyfriend is avoiding me, and my mom keeps calling me selfish and saying I’m ruining the wedding before it even starts. A few family members are also weighing in, saying I should let him do it to keep the peace. But this is my wedding, and I feel like I should have the final say. AITAH?\"\"\"\n",
    "\n",
    "prediction, confidence = predict_verdict(text, model, tokenizer)\n",
    "verdict = \"NTA\" if prediction == 1 else \"YTA\"\n",
    "print(f\"Verdict: {verdict} (confidence: {confidence:.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verdict: YTA (confidence: 0.81)\n"
     ]
    }
   ],
   "source": [
    "# https://www.reddit.com/r/AmItheAsshole/comments/1gw9o5g/aita_for_defending_my_daughters_comments_towards/\n",
    "text = \"\"\"AITA for defending my daughters comments towards my other daughter being single?\n",
    "\n",
    "My daughter (17f, Emily) has been dating this boy (17m, Zach) for around 2 -2 1/2 months now and he recently came to visit us, and this is the first time he has came over for dinner, and this is Emilys first bf. Zach is a very lovely boy and very outgoing. When he came in and saw me he says \"Emily, I didn't know you have 2 sisters\". I laughed as even though it's cliche I know he's trying to be nice. The entire time at dinner he was very polite but he is also a very outgoing kid. He would say stuff such as what a lovely dinner, this food is great, your backyard is beautiful, etc, etc. So while you could say he was trying to be overly polite, he was still a very sweet and kind kid. Emily is a more shy and reserved person so I felt they were really great for each other. Emily is also very sweet and positive, another thing they have in common that I appreciated. My husband also hit it off with him and they were engaging in sports banter, and eventually came to trash talking some football team owner.\n",
    "\n",
    "My older daughter (amy,19), however kept grilling the poor guy. Asking if he would pay for dates, to which he said yeah, and then she asks how he has money, and he said his job, then she started talking about making time for Emily, in between school friends and a job. Then it came onto how they would get to dates and she started asking him about his license, she then started to ask about protecting her making comments on his stature (hes on the shorter side and kind of chubby, like 5'7 and maybe a little overweight, nothing crazy however) and he seemed to be getting uncomfortable so I brought out dessert, which he again complimented, and my husband brought up sports to change topics.\n",
    "\n",
    "After he left I asked her why she would do that. She said that he seemed to nice, and cliche, as if he was faking it. I said so people cant be nice these days? You made it weird for him and Emily, Emily didn't deserve that neither did he. She said that she just didn't like that vibe as no-one is that nice or positive it was definetely forced. Emily butted in and said that she really didnt appreciate that and said that Amy's reasoning didn't make sense. Amy said that she didn't care if it was awkward as she wanted to grill him, and that she doesn't like him because he seemed fake. Emily said, that Amy was messed up and I agreed. Amy then said that he was some dumb weak kid faking being nice, and this upset Emily, and me. Emily then said in a fuss \"You only say that because your single and no one will date you\". She has been slightly sensitive about this as she hasn't been in a committed relationship yet.This upset Amy and Amy asked why I didn't say anything or stop her from going to her room. I said that she just insulted her bf and that she deserved it, she told me I should punish her and was being a bad parent and now Amy isn't talking to me and I feel that maybe a personal insult like that was to far.\"\"\"\n",
    "\n",
    "prediction, confidence = predict_verdict(text, model, tokenizer)\n",
    "verdict = \"NTA\" if prediction == 1 else \"YTA\"\n",
    "print(f\"Verdict: {verdict} (confidence: {confidence:.2f})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
